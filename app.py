# Copyright (c) Alibaba, Inc. and its affiliates.
import enum
import os
import shutil
import slugify
import time
from concurrent.futures import ProcessPoolExecutor
import uuid as UUID
from typing import Tuple
from torch import multiprocessing
import cv2
import gradio as gr
import numpy as np
import torch
from glob import glob
from modelscope import snapshot_download

from facechain.inference import GenPortrait
from facechain.inference_inpaint import GenPortraitInpaint
from facechain.data_process.preprocessing import get_popular_prompts
from facechain.train_text_to_image_lora import prepare_dataset, data_process_fn
from facechain.constants import neg_prompt, pos_prompt_with_cloth, pos_prompt_with_style, styles, cloth_prompt, \
    pose_models, pose_examples, base_models

training_done_count = 0
inference_done_count = 0

is_spaces = True if "SPACE_ID" in os.environ else False
if is_spaces:
    is_shared_ui = True if "modelscope/FaceChain" in os.environ['SPACE_ID'] else False
else:
    is_shared_ui = False
is_gpu_associated = torch.cuda.is_available()
class UploadTarget(enum.Enum):
    PERSONAL_PROFILE = 'Personal Profile'
    LORA_LIaBRARY = 'LoRA Library'


def update_cloth(style_index):
    style = styles[style_index]
    prompts = []
    if style_index == 0:
        example_prompt = generate_pos_prompt(style['name'],
                                             cloth_prompt[0]['prompt'])
        multiplier_human = 0.95
        for prompt in cloth_prompt:
            prompts.append(prompt['name'])
    else:
        example_prompt = generate_pos_prompt(style['name'],
                                             style['add_prompt_style'])
        multiplier_human = style['multiplier_human']
        prompts.append(style['cloth_name'])
    return gr.Radio.update(choices=prompts,
                           value=prompts[0], visible=True), gr.Textbox.update(value=example_prompt), gr.Slider.update(value=multiplier_human)


def update_prompt(style_index, cloth_index):
    style = styles[style_index]
    if style_index == 0:
        pos_prompt = generate_pos_prompt(style['name'],
                                         cloth_prompt[cloth_index]['prompt'])
        multiplier_style = 0.25
    else:
        pos_prompt = generate_pos_prompt(style['name'],
                                         style['add_prompt_style'])
        multiplier_style = style['multiplier_style']
    return gr.Textbox.update(value=pos_prompt), gr.Slider.update(value=multiplier_style)

def update_pose_model(pose_image):
    if pose_image is None:
        return gr.Radio.update(value=pose_models[0]['name'])
    else:
        return gr.Radio.update(value=pose_models[1]['name'])

def update_optional_styles(base_model_index):
    style_list = base_models[base_model_index]['style_list']
    optional_styles = '\n'.join(style_list)
    return gr.Textbox.update(value=optional_styles)

def concatenate_images(images):
    heights = [img.shape[0] for img in images]
    max_width = sum([img.shape[1] for img in images])

    concatenated_image = np.zeros((max(heights), max_width, 3), dtype=np.uint8)
    x_offset = 0
    for img in images:
        concatenated_image[0:img.shape[0], x_offset:x_offset + img.shape[1], :] = img
        x_offset += img.shape[1]
    return concatenated_image


def train_lora_fn(base_model_path=None, revision=None, sub_path=None, output_img_dir=None, work_dir=None, ensemble=True, enhance_lora=False, photo_num=0):
    validation_prompt, _ = get_popular_prompts(output_img_dir)
    torch.cuda.empty_cache()
    
    lora_r = 4 if not enhance_lora else 128
    lora_alpha = 32 if not enhance_lora else 64
    max_train_steps = min(photo_num * 200, 800)
    if ensemble:
        os.system(
            f'''
                PYTHONPATH=. accelerate launch facechain/train_text_to_image_lora.py \
                --pretrained_model_name_or_path="{base_model_path}" \
                --output_dataset_name="{output_img_dir}" \
                --caption_column="text" --resolution=512 \
                --random_flip --train_batch_size=1 --gradient_accumulation_steps=4 --max_train_steps={max_train_steps} --checkpointing_steps=100 \
                --learning_rate=1e-04 --lr_scheduler="constant" --lr_warmup_steps=0 --seed=42 --output_dir="{work_dir}" \
                --lora_r={lora_r} --lora_alpha={lora_alpha} \
                --validation_prompt="{validation_prompt}" \
                --validation_steps=100 \
                --template_dir="resources/inpaint_template" \
                --template_mask \
                --merge_best_lora_based_face_id \
                --revision="{revision}" \
                --sub_path="{sub_path}" \
            '''
        )
    else:
        os.system(
            f'PYTHONPATH=. accelerate launch facechain/train_text_to_image_lora.py --pretrained_model_name_or_path={base_model_path} '
            f'--revision={revision} --sub_path={sub_path} '
            f'--output_dataset_name={output_img_dir} --caption_column="text" --resolution=512 '
            f'--random_flip --train_batch_size=1 --num_train_epochs=200 --checkpointing_steps=5000 '
            f'--learning_rate=1.5e-04 --lr_scheduler="cosine" --lr_warmup_steps=0 --seed=42 --output_dir={work_dir} '
            f'--lora_r={lora_r} --lora_alpha={lora_alpha} --lora_text_encoder_r=32 --lora_text_encoder_alpha=32 --resume_from_checkpoint="fromfacecommon"')

def generate_pos_prompt(style_model, prompt_cloth):
    if style_model == styles[0]['name'] or style_model is None:
        pos_prompt = pos_prompt_with_cloth.format(prompt_cloth)
    else:
        matched = list(filter(lambda style: style_model == style['name'], styles))
        if len(matched) == 0:
            raise ValueError(f'styles not found: {style_model}')
        matched = matched[0]
        pos_prompt = pos_prompt_with_style.format(matched['add_prompt_style'])
    return pos_prompt


def launch_pipeline(uuid,
                    pos_prompt,
                    base_model_index=None,
                    user_model=None,
                    num_images=1,
                    style_model=None,
                    multiplier_style=0.25,
                    multiplier_human=0.85,
                    pose_model=None,
                    pose_image=None
                    ):
    # Check base model
    if base_model_index == None:
        raise gr.Error('è¯·é€‰æ‹©åŸºæ¨¡å‹(Please select the base model)ï¼')
    
    # Check output model
    if not user_model:
        raise gr.Error('è¯·é€‰æ‹©äº§å‡ºæ¨¡å‹(Please select the output model)ï¼')
    
    # Check style model
    if style_model == None:
        raise gr.Error('è¯·é€‰æ‹©é£æ ¼æ¨¡å‹(Please select the style model)ï¼')

    base_model = base_models[base_model_index]['model_id']
    revision = base_models[base_model_index]['revision']
    sub_path = base_models[base_model_index]['sub_path']
    
    before_queue_size = 0
    before_done_count = inference_done_count
    style_model = styles[style_model]['name']

    if style_model == styles[0]['name']:
        style_model_path = None
    else:
        matched = list(filter(lambda style: style_model == style['name'], styles))
        if len(matched) == 0:
            raise ValueError(f'styles not found: {style_model}')
        matched = matched[0]
        model_dir = snapshot_download(matched['model_id'], revision=matched['revision'])
        style_model_path = os.path.join(model_dir, matched['bin_file'])

    if pose_image is None or pose_model == 0:
        pose_model_path = None
        use_depth_control = False
        pose_image = None
    else:
        model_dir = snapshot_download('damo/face_chain_control_model', revision='v1.0.1')
        pose_model_path = os.path.join(model_dir, 'model_controlnet/control_v11p_sd15_openpose')
        if pose_model == 1:
            use_depth_control = True
        else:
            use_depth_control = False

    print("-------user_model: ", user_model)
    if not uuid:
        uuid = UUID.uuid4().hex


    use_main_model = True
    use_face_swap = True
    use_post_process = True
    use_stylization = False

    instance_data_dir = os.path.join('/tmp', uuid, 'training_data', base_model, user_model)
    lora_model_path = f'/tmp/{uuid}/{base_model}/{user_model}/ensemble'
    if not os.path.exists(lora_model_path):
        lora_model_path = f'/tmp/{uuid}/{base_model}/{user_model}/'

    train_file = os.path.join(lora_model_path, 'pytorch_lora_weights.bin')
    if not os.path.exists(train_file):
        raise gr.Error('æ‚¨è¿˜æ²¡æœ‰è¿›è¡Œå½¢è±¡å®šåˆ¶ï¼Œè¯·å…ˆè¿›è¡Œè®­ç»ƒã€‚(Training is required before inference.)')


    gen_portrait = GenPortrait(pose_model_path, pose_image, use_depth_control, pos_prompt, neg_prompt, style_model_path, 
                               multiplier_style, multiplier_human, use_main_model,
                               use_face_swap, use_post_process,
                               use_stylization)

    num_images = min(6, num_images)

    with ProcessPoolExecutor(max_workers=5) as executor:
        future = executor.submit(gen_portrait, instance_data_dir,
                                            num_images, base_model, lora_model_path, sub_path, revision)
        while not future.done():
            is_processing = future.running()
            if not is_processing:
                cur_done_count = inference_done_count
                to_wait = before_queue_size - (cur_done_count - before_done_count)
                yield [uuid, "æ’é˜Ÿç­‰å¾…èµ„æºä¸­ï¼Œå‰æ–¹è¿˜æœ‰{}ä¸ªç”Ÿæˆä»»åŠ¡, é¢„è®¡éœ€è¦ç­‰å¾…{}åˆ†é’Ÿ...".format(to_wait, to_wait * 2.5),
                        None]
            else:
                yield [uuid, "ç”Ÿæˆä¸­, è¯·è€å¿ƒç­‰å¾…(Generating)...", None]
            time.sleep(1)

    outputs = future.result()
    outputs_RGB = []
    for out_tmp in outputs:
        outputs_RGB.append(cv2.cvtColor(out_tmp, cv2.COLOR_BGR2RGB))
    image_path = './lora_result.png'
    if len(outputs) > 0:
        result = concatenate_images(outputs)
        cv2.imwrite(image_path, result)

        yield [uuid, "ç”Ÿæˆå®Œæ¯•(Generation done)ï¼", outputs_RGB]
    else:
        yield [uuid, "ç”Ÿæˆå¤±è´¥ï¼Œè¯·é‡è¯•(Generation failed, please retry)ï¼", outputs_RGB]


def launch_pipeline_inpaint(uuid,
                          base_model_index=None,
                          user_model=None,
                          selected_template_images=None,
                          append_pos_prompt=None,
                          select_face_num=1,
                          first_control_weight=0.5,
                          second_control_weight=0.1,
                          final_fusion_ratio=0.5,
                          use_fusion_before=True,
                          use_fusion_after=True):
    before_queue_size = 0
    before_done_count = inference_done_count

    # Check base model
    if base_model_index == None:
        raise gr.Error('è¯·é€‰æ‹©åŸºæ¨¡å‹(Please select the base model)ï¼')
    
    # Check output model
    if not user_model:
        raise gr.Error('è¯·é€‰æ‹©äº§å‡ºæ¨¡å‹(Please select the output model)ï¼')

    if not uuid:
        if os.getenv("MODELSCOPE_ENVIRONMENT") == 'studio':
            return "è¯·ç™»é™†åä½¿ç”¨! (Please login first)"
        else:
            uuid = 'qw'

    if isinstance(selected_template_images, str):
        if len(selected_template_images) == 0:
            raise gr.Error('è¯·é€‰æ‹©ä¸€å¼ æ¨¡æ¿(Please select 1 template)')

    base_model = base_models[base_model_index]['model_id']
    revision = base_models[base_model_index]['revision']
    sub_path = base_models[base_model_index]['sub_path']
    output_model_name = 'personalization_lora'
    instance_data_dir = os.path.join('/tmp', uuid, 'training_data', base_model, user_model)

    # we use ensemble model, if not exists fallback to original lora
    lora_model_path = f'/tmp/{uuid}/{base_model}/{user_model}/ensemble/'
    if not os.path.exists(lora_model_path):
        lora_model_path = f'/tmp/{uuid}/{base_model}/{user_model}/'

    gen_portrait_inpaint = GenPortraitInpaint(crop_template=False, short_side_resize=512)
    
    cache_model_dir = snapshot_download("bubbliiiing/controlnet_helper", revision="v2.2")

    with ProcessPoolExecutor(max_workers=5) as executor:
        future = executor.submit(gen_portrait_inpaint, base_model, lora_model_path, instance_data_dir, \
                                 selected_template_images, cache_model_dir, select_face_num, first_control_weight, \
                                 second_control_weight, final_fusion_ratio, use_fusion_before, use_fusion_after, \
                                 sub_path=sub_path, revision=revision)

        while not future.done():
            is_processing = future.running()
            if not is_processing:
                cur_done_count = inference_done_count
                to_wait = before_queue_size - (cur_done_count - before_done_count)
                yield ["æ’é˜Ÿç­‰å¾…èµ„æºä¸­ï¼Œå‰æ–¹è¿˜æœ‰{}ä¸ªç”Ÿæˆä»»åŠ¡, é¢„è®¡éœ€è¦ç­‰å¾…{}åˆ†é’Ÿ...".format(to_wait, to_wait * 2.5),
                       None]
            else:
                yield ["ç”Ÿæˆä¸­, è¯·è€å¿ƒç­‰å¾…(Generating)...", None]
            time.sleep(1)

    outputs = future.result()
    outputs_RGB = []
    for out_tmp in outputs:
        outputs_RGB.append(cv2.cvtColor(out_tmp, cv2.COLOR_BGR2RGB))
    image_path = './lora_result.png'
    if len(outputs) > 0:
        result = concatenate_images(outputs)
        cv2.imwrite(image_path, result)

        yield ["ç”Ÿæˆå®Œæ¯•(Generation done)ï¼", outputs_RGB]
    else:
        yield ["ç”Ÿæˆå¤±è´¥ï¼Œè¯·é‡è¯•(Generation failed, please retry)ï¼", outputs_RGB]


class Trainer:
    def __init__(self):
        pass

    def run(
            self,
            uuid: str,
            ensemble: bool,
            enhance_lora: bool,
            instance_images: list,
            base_model_index: int,
            output_model_name: str,
    ) -> Tuple[str, str]:
        # Check Cuda
        if not torch.cuda.is_available():
            raise gr.Error('CUDAä¸å¯ç”¨(CUDA not available)')

        # Check Instance Valid
        if instance_images is None:
            raise gr.Error('æ‚¨éœ€è¦ä¸Šä¼ è®­ç»ƒå›¾ç‰‡(Please upload photos)ï¼')
        
        # Check output model name
        if not output_model_name:
            raise gr.Error('è¯·æŒ‡å®šäº§å‡ºæ¨¡å‹çš„åç§°(Please specify the output model name)ï¼')
        
        # Limit input Image
        if len(instance_images) > 20:
            raise gr.Error('è¯·æœ€å¤šä¸Šä¼ 20å¼ è®­ç»ƒå›¾ç‰‡(20 images at most!)')

        # Check UUID & Studio
        if not uuid:
            uuid = UUID.uuid4().hex


        base_model_path = base_models[base_model_index]['model_id']
        revision = base_models[base_model_index]['revision']
        sub_path = base_models[base_model_index]['sub_path']
        output_model_name = slugify.slugify(output_model_name)

        # mv user upload data to target dir
        instance_data_dir = os.path.join('/tmp', uuid, 'training_data', base_model_path, output_model_name)
        print("--------uuid: ", uuid)

        if not os.path.exists(f"/tmp/{uuid}"):
            os.makedirs(f"/tmp/{uuid}")
        work_dir = f"/tmp/{uuid}/{base_model_path}/{output_model_name}"

        if os.path.exists(work_dir):
            raise gr.Error("äº§å‡ºæ¨¡å‹åç§°å·²å­˜åœ¨ã€‚")

        print("----------work_dir: ", work_dir)
        shutil.rmtree(work_dir, ignore_errors=True)
        shutil.rmtree(instance_data_dir, ignore_errors=True)

        prepare_dataset([img['name'] for img in instance_images], output_dataset_dir=instance_data_dir)
        data_process_fn(instance_data_dir, True)

        # train lora
        print("instance_data_dir", instance_data_dir)
        train_lora_fn(base_model_path=base_model_path,
                      revision=revision,
                      sub_path=sub_path,
                      output_img_dir=instance_data_dir,
                      work_dir=work_dir,
                      ensemble=ensemble,
                      enhance_lora=enhance_lora,
                      photo_num=len(instance_images))

        message = f'è®­ç»ƒå·²ç»å®Œæˆï¼è¯·åˆ‡æ¢è‡³ [å½¢è±¡ä½“éªŒ] æ ‡ç­¾ä½“éªŒæ¨¡å‹æ•ˆæœ(Training done, please switch to the inference tab to generate photos.)'
        print(message)
        return uuid, message


def flash_model_list(uuid, base_model_index):
    base_model_path = base_models[base_model_index]['model_id']
    style_list = base_models[base_model_index]['style_list']

    if not uuid:
        if os.getenv("MODELSCOPE_ENVIRONMENT") == 'studio':
            return "è¯·ç™»é™†åä½¿ç”¨! (Please login first)"
        else:
            uuid = 'qw'

    folder_path = f"/tmp/{uuid}/{base_model_path}"
    folder_list = []
    if not os.path.exists(folder_path):
        return gr.Radio.update(choices=[]),gr.Dropdown.update(choices=style_list)
    else:
        files = os.listdir(folder_path)
        for file in files:
            file_path = os.path.join(folder_path, file)
            if os.path.isdir(folder_path):
                file_lora_path = f"{file_path}/pytorch_lora_weights.bin"
                if os.path.exists(file_lora_path):
                    folder_list.append(file)

    return gr.Radio.update(choices=folder_list), gr.Dropdown.update(choices=style_list, value=style_list[0], visible=True)

def update_output_model(uuid, base_model_index):

    # Check base model
    if base_model_index == None:
        raise gr.Error('è¯·é€‰æ‹©åŸºæ¨¡å‹(Please select the base model)ï¼')

    base_model_path = base_models[base_model_index]['model_id']

    if not uuid:
        if os.getenv("MODELSCOPE_ENVIRONMENT") == 'studio':
            return "è¯·ç™»é™†åä½¿ç”¨! (Please login first)"
        else:
            uuid = 'qw'

    folder_path = f"/tmp/{uuid}/{base_model_path}"
    folder_list = []
    if not os.path.exists(folder_path):
        return gr.Radio.update(choices=[]),gr.Dropdown.update(choices=style_list)
    else:
        files = os.listdir(folder_path)
        for file in files:
            file_path = os.path.join(folder_path, file)
            if os.path.isdir(folder_path):
                file_lora_path = f"{file_path}/pytorch_lora_weights.bin"
                if os.path.exists(file_lora_path):
                    folder_list.append(file)

    return gr.Radio.update(choices=folder_list)


def upload_file(files, current_files):
    file_paths = [file_d['name'] for file_d in current_files] + [file.name for file in files]
    return file_paths


def train_input(uuid):
    trainer = Trainer()
    with gr.Blocks() as demo:
        with gr.Row():
            with gr.Column():
                with gr.Box():
                    gr.Markdown('æ¨¡å‹é€‰æ‹©(Model list)')

                    base_model_list = []
                    for base_model in base_models:
                        base_model_list.append(base_model['name'])

                    base_model_index = gr.Radio(label="åŸºæ¨¡å‹é€‰æ‹©(Base model list)", choices=base_model_list, type="index",
                                       value=base_model_list[0])
                    
                    optional_style = '\n'.join(base_models[0]['style_list'])
                    
                    optional_styles = gr.Textbox(label="è¯¥åŸºæ¨¡å‹æ”¯æŒçš„é£æ ¼(Styles supported by this base model.)", lines=3,
                                        value=optional_style, interactive=False)
                    
                    output_model_name = gr.Textbox(label="äº§å‡ºæ¨¡å‹åç§°(Output model name)", value='test', lines=1)
 
                    gr.Markdown('è®­ç»ƒå›¾ç‰‡(Training photos)')
                    instance_images = gr.Gallery()
                    upload_button = gr.UploadButton("é€‰æ‹©å›¾ç‰‡ä¸Šä¼ (Upload photos)", file_types=["image"],
                                                    file_count="multiple")

                    clear_button = gr.Button("æ¸…ç©ºå›¾ç‰‡(Clear photos)")
                    clear_button.click(fn=lambda: [], inputs=None, outputs=instance_images)

                    upload_button.upload(upload_file, inputs=[upload_button, instance_images], outputs=instance_images,
                                         queue=False)
                    
                    gr.Markdown('''
                        - Step 1. ä¸Šä¼ è®¡åˆ’è®­ç»ƒçš„å›¾ç‰‡ï¼Œ3~10å¼ å¤´è‚©ç…§ï¼ˆæ³¨æ„ï¼šè¯·é¿å…å›¾ç‰‡ä¸­å‡ºç°å¤šäººè„¸ã€è„¸éƒ¨é®æŒ¡ç­‰æƒ…å†µï¼Œå¦åˆ™å¯èƒ½å¯¼è‡´æ•ˆæœå¼‚å¸¸ï¼‰
                        - Step 2. ç‚¹å‡» [å¼€å§‹è®­ç»ƒ] ï¼Œå¯åŠ¨å½¢è±¡å®šåˆ¶åŒ–è®­ç»ƒï¼Œçº¦éœ€15åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…ï½
                        - Step 3. åˆ‡æ¢è‡³ [å½¢è±¡ä½“éªŒ] ï¼Œç”Ÿæˆä½ çš„é£æ ¼ç…§ç‰‡
                        ''')
                    gr.Markdown('''
                        - Step 1. Upload 3-10 headshot photos of yours (Note: avoid photos with multiple faces or face obstruction, which may lead to non-ideal result).
                        - Step 2. Click [Train] to start training for customizing your Digital-Twin, this may take up-to 15 mins.
                        - Step 3. Switch to [Inference] Tab to generate stylized photos.
                        ''')

        with gr.Box():
            with gr.Row():
                ensemble = gr.Checkbox(label='äººç‰©LoRAèåˆï¼ˆEnsembleï¼‰', value=False)
                enhance_lora = gr.Checkbox(label='LoRAå¢å¼ºï¼ˆLoRA-Enhancementï¼‰', value=False)
            gr.Markdown(
                '''
                - äººç‰©LoRAèåˆï¼ˆEnsembleï¼‰ï¼šé€‰æ‹©è®­ç»ƒä¸­å‡ ä¸ªæœ€ä½³äººç‰©LoRAèåˆã€‚æå‡ç›¸ä¼¼åº¦æˆ–åœ¨è‰ºæœ¯ç…§ç”Ÿæˆæ¨¡å¼ä¸‹å»ºè®®å‹¾é€‰ - Allow fusion of multiple LoRAs during training. Recommended for enhanced-similarity or using with Inpaint mode.
                - LoRAå¢å¼ºï¼ˆLoRA-Enhancementï¼‰ï¼šæ‰©å¤§LoRAè§„æ¨¡ï¼Œç”Ÿæˆå›¾ç‰‡æ›´è´´è¿‘ç”¨æˆ·ï¼Œè‡³å°‘5å¼ ä»¥ä¸Šå¤šå›¾è®­ç»ƒæˆ–è€…è‰ºæœ¯ç…§ç”Ÿæˆæ¨¡å¼å»ºè®®å‹¾é€‰ - Boost scale of LoRA to enhance output resemblance with input. Recommended for training with more than 5 pics or using with Inpaint mode. 
                '''
            )

        run_button = gr.Button('å¼€å§‹è®­ç»ƒï¼ˆç­‰å¾…ä¸Šä¼ å›¾ç‰‡åŠ è½½æ˜¾ç¤ºå‡ºæ¥å†ç‚¹ï¼Œå¦åˆ™ä¼šæŠ¥é”™ï¼‰'
                               'Start training (please wait until photo(s) fully uploaded, otherwise it may result in training failure)')

        with gr.Box():
            gr.Markdown('''
            è¯·ç­‰å¾…è®­ç»ƒå®Œæˆï¼Œè¯·å‹¿åˆ·æ–°æˆ–å…³é—­é¡µé¢ã€‚
            
            Please wait for the training to complete, do not refresh or close the page.
            ''')
            output_message = gr.Markdown()
        with gr.Box():
            gr.Markdown('''
            ç¢°åˆ°æŠ“ç‹‚çš„é”™è¯¯æˆ–è€…è®¡ç®—èµ„æºç´§å¼ çš„æƒ…å†µä¸‹ï¼Œæ¨èç›´æ¥åœ¨[NoteBook](https://modelscope.cn/my/mynotebook/preset)ä¸Šè¿›è¡Œä½“éªŒã€‚
            
            å®‰è£…æ–¹æ³•è¯·å‚è€ƒï¼šhttps://github.com/modelscope/facechain .
            
            If you are experiencing prolonged waiting time, you may try on [ModelScope NoteBook](https://modelscope.cn/my/mynotebook/preset) to prepare your dedicated environment.
                        
            You may refer to: https://github.com/modelscope/facechain for installation instruction.
            ''')
        base_model_index.change(fn=update_optional_styles,
                                inputs=[base_model_index],
                                outputs=[optional_styles],
                                queue=False)

        run_button.click(fn=trainer.run,
                         inputs=[
                             uuid,
                             ensemble,
                             enhance_lora,
                             instance_images,
                             base_model_index,
                             output_model_name,
                         ],
                         outputs=[uuid, output_message])

    return demo


def inference_input(uuid):
    with gr.Blocks() as demo:
        with gr.Row():
            with gr.Column():
                base_model_list = []
                for base_model in base_models:
                    base_model_list.append(base_model['name'])

                base_model_index = gr.Radio(label="åŸºæ¨¡å‹é€‰æ‹©(Base model list)", choices=base_model_list, type="index")
                
                with gr.Row():
                    with gr.Column(scale=3):
                        user_model = gr.Radio(label="äº§å‡ºæ¨¡å‹(Output Model list)", choices=[], type="value")
                    with gr.Column(scale=1):
                        update_button = gr.Button('åˆ·æ–°äº§å‡ºæ¨¡å‹(Update output model list)')

                style_model_list = []
                for style in styles:
                    style_model_list.append(style['name'])
                style_model = gr.Dropdown(choices=style_model_list, type="index", value=style_model_list[0], 
                                          label="é£æ ¼æ¨¡å‹(Style model)", visible=False)
                
                prompts = []
                for prompt in cloth_prompt:
                    prompts.append(prompt['name'])
                for style in styles[1:]:
                    prompts.append(style['cloth_name'])

                cloth_style = gr.Radio(choices=prompts, value=cloth_prompt[0]['name'],
                                       type="index", label="æœè£…é£æ ¼(Cloth style)", visible=False)
                pmodels = []
                for pmodel in pose_models:
                    pmodels.append(pmodel['name'])

                with gr.Accordion("é«˜çº§é€‰é¡¹(Advanced Options)", open=False):
                    pos_prompt = gr.Textbox(label="æç¤ºè¯­(Prompt)", lines=3, 
                                            value=generate_pos_prompt(None, cloth_prompt[0]['prompt']),
                                            interactive=True)
                    multiplier_style = gr.Slider(minimum=0, maximum=1, value=0.25,
                                                 step=0.05, label='é£æ ¼æƒé‡(Multiplier style)')
                    multiplier_human = gr.Slider(minimum=0, maximum=1.2, value=0.95,
                                                 step=0.05, label='å½¢è±¡æƒé‡(Multiplier human)')
                    pose_image = gr.Image(source='upload', type='filepath', label='å§¿æ€å›¾ç‰‡(Pose image)')
                    gr.Examples(pose_examples['man'], inputs=[pose_image], label='ç”·æ€§å§¿æ€ç¤ºä¾‹')
                    gr.Examples(pose_examples['woman'], inputs=[pose_image], label='å¥³æ€§å§¿æ€ç¤ºä¾‹')
                    pose_model = gr.Radio(choices=pmodels, value=pose_models[0]['name'],
                                          type="index", label="å§¿æ€æ§åˆ¶æ¨¡å‹(Pose control model)")
                with gr.Box():
                    num_images = gr.Number(
                        label='ç”Ÿæˆå›¾ç‰‡æ•°é‡(Number of photos)', value=6, precision=1, minimum=1, maximum=6)
                    gr.Markdown('''
                    æ³¨æ„ï¼šæœ€å¤šæ”¯æŒç”Ÿæˆ6å¼ å›¾ç‰‡!(You may generate a maximum of 6 photos at one time!)
                        ''')

        display_button = gr.Button('å¼€å§‹ç”Ÿæˆ(Start!)')

        with gr.Box():
            infer_progress = gr.Textbox(label="ç”Ÿæˆè¿›åº¦(Progress)", value="å½“å‰æ— ç”Ÿæˆä»»åŠ¡(No task)", interactive=False)
        with gr.Box():
            gr.Markdown('ç”Ÿæˆç»“æœ(Result)')
            output_images = gr.Gallery(label='Output', show_label=False).style(columns=3, rows=2, height=600,
                                                                               object_fit="contain")
                                                                               
        style_model.change(update_cloth, style_model, [cloth_style, pos_prompt, multiplier_human], queue=False)
        cloth_style.change(update_prompt, [style_model, cloth_style], [pos_prompt, multiplier_style], queue=False)
        pose_image.change(update_pose_model, pose_image, [pose_model])
        base_model_index.change(fn=flash_model_list,
                                inputs=[uuid, base_model_index],
                                outputs=[user_model, style_model],
                                queue=False)
        update_button.click(fn=update_output_model,
                      inputs=[uuid, base_model_index],
                      outputs=[user_model],
                      queue=False)
        display_button.click(fn=launch_pipeline,
                             inputs=[uuid, pos_prompt, base_model_index, user_model, num_images, style_model, multiplier_style, multiplier_human,
                                     pose_model, pose_image],
                             outputs=[uuid, infer_progress, output_images])

    return demo


def inference_inpaint():
    """
        Inpaint Tab with Ensemble-Lora + MultiControlnet, support preset_template
        #TODO: Support user upload template && template check logits
    """
    preset_template = glob(os.path.join('resources/inpaint_template/*.jpg'))
    with gr.Blocks() as demo:
        uuid = gr.Text(label="modelscope_uuid", visible=False)
        # Initialize the GUI
        
        with gr.Row():
            with gr.Column():
                base_model_list = []
                for base_model in base_models:
                    base_model_list.append(base_model['name'])

                base_model_index = gr.Radio(
                    label="åŸºæ¨¡å‹é€‰æ‹©(Base model list)",
                    choices=base_model_list,
                    type="index"
                )

                user_model = gr.Radio(label="äº§å‡ºæ¨¡å‹(Output Model list)", choices=[], type="value")
                
                template_gallery_list = [(i, f"æ¨¡æ¿{idx + 1}") for idx, i in enumerate(preset_template)]
                gallery = gr.Gallery(template_gallery_list).style(grid=4, height=300)

                # new inplementation with gr.select callback function, only pick 1image at once
                def select_function(evt: gr.SelectData):
                    return [preset_template[evt.index]]

                selected_template_images = gr.Text(show_label=False, placeholder="Selected")
                gallery.select(select_function, None, selected_template_images)
                
                with gr.Accordion("é«˜çº§é€‰é¡¹(Advanced Options)", open=False):
                    append_pos_prompt = gr.Textbox(
                        label="æç¤ºè¯­(Prompt)",
                        lines=3,
                        value='masterpiece, smile, beauty',
                        interactive=True
                    )
                    first_control_weight = gr.Slider(
                        minimum=0.35, maximum=0.6, value=0.45,
                        step=0.02, label='åˆå§‹æƒé‡(Initial Control Weight)'
                    )

                    second_control_weight = gr.Slider(
                        minimum=0.04, maximum=0.2, value=0.1,
                        step=0.02, label='äºŒæ¬¡æƒé‡(Secondary Control Weight)'
                    )
                    final_fusion_ratio = gr.Slider(
                        minimum=0.2, maximum=0.8, value=0.5,
                        step=0.1, label='èåˆç³»æ•°(Final Fusion Ratio)'
                    )
                    select_face_num = gr.Slider(
                        minimum=1, maximum=4, value=1,
                        step=1, label='ç”Ÿæˆæ•°ç›®(Number of Reference Faces)'
                    )
                    use_fusion_before = gr.Radio(
                        label="å‰èåˆ(Apply Fusion Before)", type="value", choices=[True, False],
                        value=True
                    )
                    use_fusion_after = gr.Radio(
                        label="åèåˆ(Apply Fusion After)", type="value", choices=[True, False],
                        value=True
                    )
                
        display_button = gr.Button('å¼€å§‹ç”Ÿæˆ(Start Generation)')
        with gr.Box():
            infer_progress = gr.Textbox(
                label="ç”Ÿæˆ(Generation Progress)",
                value="No task currently",
                interactive=False
            )
        with gr.Box():
            gr.Markdown('ç”Ÿæˆç»“æœ(Generated Results)')
            output_images = gr.Gallery(
                label='è¾“å‡º(Output)',
                show_label=False
            ).style(columns=3, rows=2, height=600, object_fit="contain")

        base_model_index.change(fn=update_output_model,
                                inputs=[uuid, base_model_index],
                                outputs=[user_model],
                                queue=False)
                        
        display_button.click(
            fn=launch_pipeline_inpaint,
            inputs=[uuid, base_model_index, user_model, selected_template_images, append_pos_prompt, select_face_num, first_control_weight,
                    second_control_weight,
                    final_fusion_ratio, use_fusion_before, use_fusion_after],
            outputs=[infer_progress, output_images]
        )
        
    return demo


with gr.Blocks(css='style.css') as demo:
	gr.Markdown("# <center> \N{fire} FaceChain Potrait Generation ([Github star it here](https://github.com/modelscope/facechain/tree/main) \N{whale}, [Paper cite it here](https://arxiv.org/abs/2308.14256) \N{whale})</center>")
    with gr.Box():
        if is_shared_ui:
            top_description = gr.HTML(f'''
                <div class="gr-prose" style="max-width: 80%">
                <p>If the waiting queue is too long, you can either run locally or duplicate the Space and run it on your own profile using a (paid) private A10G-large GPU for training. A A10G-large costs US$3.15/h. &nbsp;&nbsp;<a class="duplicate-button" style="display:inline-block" target="_blank" href="https://huggingface.co/spaces/{os.environ['SPACE_ID']}?duplicate=true"><img src="https://img.shields.io/badge/-Duplicate%20Space-blue?labelColor=white&style=flat&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAAXNSR0IArs4c6QAAAP5JREFUOE+lk7FqAkEURY+ltunEgFXS2sZGIbXfEPdLlnxJyDdYB62sbbUKpLbVNhyYFzbrrA74YJlh9r079973psed0cvUD4A+4HoCjsA85X0Dfn/RBLBgBDxnQPfAEJgBY+A9gALA4tcbamSzS4xq4FOQAJgCDwV2CPKV8tZAJcAjMMkUe1vX+U+SMhfAJEHasQIWmXNN3abzDwHUrgcRGmYcgKe0bxrblHEB4E/pndMazNpSZGcsZdBlYJcEL9Afo75molJyM2FxmPgmgPqlWNLGfwZGG6UiyEvLzHYDmoPkDDiNm9JR9uboiONcBXrpY1qmgs21x1QwyZcpvxt9NS09PlsPAAAAAElFTkSuQmCC&logoWidth=14" alt="Duplicate Space"></a></p>
                <img style="position: absolute; top: 0;right: 0; height: 100%;margin-top: 0px !important" src="file=duplicate.png"> 
                </div>
            ''')
        elif is_spaces:
            if is_gpu_associated:
                top_description = gr.HTML(f'''
                                <div class="gr-prose" style="max-width: 80%">
                                <h2>You have successfully associated a GPU to the FaceChain Space ğŸ‰</h2>
                                <p>You can now train your model! You will be billed by the minute from when you activated the GPU until when it is turned it off.</p> 
                                </div>
                        ''')
            else:
                top_description = gr.HTML(f'''
                                <div class="gr-prose" style="max-width: 80%">
                                <h2>You have successfully duplicated the FaceChain Space ğŸ‰</h2>
                                <p>There's only one step left before you can train your model: <a href="https://huggingface.co/spaces/{os.environ['SPACE_ID']}/settings" style="text-decoration: underline" target="_blank">attribute a <b>A10G-large GPU</b> to it (via the Settings tab)</a> and run the training below. You will be billed by the minute from when you activate the GPU until when it is turned it off.</p> 
                                </div>
                        ''')
        else:
            top_description = gr.HTML(f'''
                            <div class="gr-prose" style="max-width: 80%">
                            <h2>You have successfully cloned the FaceChain Space locally ğŸ‰</h2>
                            <p>Do a <code>pip install requirements.txt</code></p> 
                            </div>
                        ''')
    uuid = gr.State([])
    with gr.Tabs():
        with gr.TabItem('\N{rocket}å½¢è±¡å®šåˆ¶(Train)'):
            train_input(uuid)
        with gr.TabItem('\N{party popper}å½¢è±¡ä½“éªŒ(Inference)'):
            inference_input(uuid)
        with gr.TabItem('\N{party popper}è‰ºæœ¯ç…§(Inpaint)'):
            inference_inpaint(uuid)

if __name__ == "__main__":
    multiprocessing.set_start_method('spawn')
    demo.queue(status_update_rate=1).launch(share=True)

