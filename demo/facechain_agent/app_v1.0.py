from __future__ import annotations
import os
import sys
sys.path.append('../../')
from functools import partial
import json
import shutil
import slugify
import glob
from torch import multiprocessing
import gradio as gr
from dotenv import load_dotenv
from modelscope_agent.agent import AgentExecutor
from modelscope_agent.llm import LLMFactory
from modelscope_agent.prompt import MSPromptGenerator, PromptGenerator
from modelscope_agent.retrieve import ToolRetrieval
from gradio_chatbot import ChatBot
from help_tool import StyleSearchTool, FaceChainFineTuneTool, FaceChainInferenceTool
import copy
from facechain.train_text_to_image_lora import prepare_dataset
from modelscope.utils.config import Config
import uuid
import dashscope
from dashscope.audio.tts import SpeechSynthesizer
from dashscope.audio.asr import Recognition
import subprocess

PROMPT_START = "ä½ å¥½ï¼Œæˆ‘æ˜¯FaceChainAgentï¼Œå¯ä»¥å¸®ä½ ç”Ÿæˆå†™çœŸç…§ç‰‡ã€‚è¯·å‘Šè¯‰æˆ‘ä½ éœ€è¦çš„é£æ ¼çš„åå­—ã€‚"

SYSTEM_PROMPT = """<|system|>: ä½ ç°åœ¨æ‰®æ¼”ä¸€ä¸ªFacechain Agentï¼Œä¸æ–­å’Œç”¨æˆ·æ²Ÿé€šåˆ›ä½œæƒ³æ³•ï¼Œè¯¢é—®ç”¨æˆ·å†™çœŸç…§é£æ ¼ï¼Œæœ€åç”Ÿæˆæœç´¢åˆ°çš„é£æ ¼ç±»å‹è¿”å›ç»™ç”¨æˆ·ã€‚å½“å‰å¯¹è¯å¯ä»¥ä½¿ç”¨çš„æ’ä»¶ä¿¡æ¯å¦‚ä¸‹ï¼Œè¯·è‡ªè¡Œåˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨æ’ä»¶æ¥è§£å†³å½“å‰ç”¨æˆ·é—®é¢˜ã€‚è‹¥éœ€è¦è°ƒç”¨æ’ä»¶ï¼Œåˆ™éœ€è¦å°†æ’ä»¶è°ƒç”¨è¯·æ±‚æŒ‰ç…§jsonæ ¼å¼ç»™å‡ºï¼Œå¿…é¡»åŒ…å«api_nameã€parameterså­—æ®µï¼Œå¹¶åœ¨å…¶å‰åä½¿ç”¨<|startofthink|>å’Œ<|endofthink|>ä½œä¸ºæ ‡å¿—ã€‚ç„¶åä½ éœ€è¦æ ¹æ®æ’ä»¶APIè°ƒç”¨ç»“æœç”Ÿæˆåˆç†çš„ç­”å¤ã€‚
\n<tool_list>\n"""

INSTRUCTION_TEMPLATE = """ã€å¤šè½®å¯¹è¯å†å²ã€‘
<|user|>: ç»™æˆ‘ç”Ÿæˆä¸€ä¸ªèµ›åšæœ‹å…‹é£çš„å§

<|assistant|>: å¥½çš„ï¼Œæˆ‘å°†ä¸ºæ‚¨æ‰¾åˆ°è¿™ä¸ªé£æ ¼ç±»å‹ã€‚
æ­£åœ¨æœç´¢é£æ ¼ç±»å‹ï¼š
<|startofthink|>```JSON\n{\n   "api_name": "style_search_tool",\n    "parameters": {\n      "text": "æˆ‘æƒ³è¦èµ›åšæœ‹å…‹é£ã€‚"\n   }\n}\n```<|endofthink|>

<|startofexec|>```JSON\n{"result": {"name": "style_search_tool", "value": "èµ›åšæœ‹å…‹(Cybernetics punk)", file_path: "../../styles/leosamsMoonfilm_filmGrain20/Cybernetics_punk.json"}}\n```<|endofexec|>

æˆ‘å·²ä¸ºä½ æ‰¾åˆ°çš„é£æ ¼ç±»å‹åå­—æ˜¯èµ›åšæœ‹å…‹(Cybernetics punk)ã€‚ä¸‹é¢æ˜¯è¯¥é£æ ¼çš„é¢„è§ˆå›¾ã€‚

ç°åœ¨æˆ‘éœ€è¦ä½ æä¾›1-3å¼ ç…§ç‰‡ï¼Œè¯·ç‚¹å‡»å›¾ç‰‡ä¸Šä¼ æŒ‰é’®ä¸Šä¼ ä½ çš„ç…§ç‰‡ã€‚ä¸Šä¼ å®Œæ¯•ååœ¨å¯¹è¯æ¡†é‡Œå‘Šè¯‰æˆ‘ä½ å·²ç»ä¸Šä¼ å¥½ç…§ç‰‡äº†ã€‚\n\n</s>

<|user|>: æˆ‘çš„ç…§ç‰‡ä¸Šä¼ å¥½äº†ã€‚

<|assistant|>: æ”¶åˆ°ï¼Œæˆ‘éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´è®­ç»ƒä½ ä¸Šä¼ çš„ç…§ç‰‡ï¼Œç„¶åå†ç”Ÿæˆæ‚¨çš„èµ›åšæœ‹å…‹é£æ ¼å†™çœŸç…§ã€‚

æ­£åœ¨è®­ç»ƒäººç‰©loraä¸­ï¼š
<|startofthink|>```JSON\n{\n   "api_name": "facechain_finetune_tool",\n    "parameters": {}\n}\n```<|endofthink|>

äººç‰©loraè®­ç»ƒå®Œæˆã€‚æ­£åœ¨ç”Ÿæˆä½ é€‰æ‹©çš„èµ›åšæœ‹å…‹é£æ ¼å†™çœŸç…§ä¸­ï¼š
<|startofthink|>```JSON\n{\n   "api_name": "facechain_inference_tool",\n    "parameters": {\n   "matched_style_file_path": "../../styles/leosamsMoonfilm_filmGrain20/Cybernetics_punk.json"\n  }\n}\n```<|endofthink|>

å†™çœŸç…§å·²ç»ç”Ÿæˆå®Œæ¯•ï¼ä½ è¿˜å¯ä»¥ç»§ç»­ç”Ÿæˆè¿™ç±»é£æ ¼çš„ç…§ç‰‡æˆ–è€…æ›´æ¢ä¸€ä¸ªé£æ ¼\n\n</s>

ã€è§’è‰²æ‰®æ¼”è¦æ±‚ã€‘
ä¸Šé¢å¤šè½®è§’è‰²å¯¹è¯æ˜¯æä¾›çš„åˆ›ä½œä¸€ä¸ªå†™çœŸç…§é£æ ¼è¦å’Œç”¨æˆ·æ²Ÿé€šçš„æ ·ä¾‹ï¼Œè¯·æŒ‰ç…§ä¸Šè¿°çš„è¯¢é—®æ­¥éª¤æ¥å¼•å¯¼ç”¨æˆ·å®Œæˆé£æ ¼çš„ç”Ÿæˆï¼Œæ¯æ¬¡åªå›å¤å¯¹åº”çš„å†…å®¹ï¼Œä¸è¦ç”Ÿæˆå¤šè½®å¯¹è¯ã€‚è®°ä½åªå›å¤ç”¨æˆ·å½“å‰çš„æé—®ï¼Œä¸è¦ç”Ÿæˆå¤šè½®å¯¹è¯ï¼Œå›å¤ä¸è¦åŒ…å«<|user|>åé¢çš„å†…å®¹ã€‚
"""


INSTRUCTION_TEMPLATE1 = """
<|user|>: æˆ‘æƒ³è¦æ¢ä¸ªå¤é£é£æ ¼ã€‚

<|assistant|>: å¥½çš„ï¼Œæˆ‘å°†é¦–å…ˆæœç´¢ç›¸å…³é£æ ¼ï¼Œç„¶åå†ä¸ºæ‚¨ç”Ÿæˆå¤é£é£æ ¼çš„å†™çœŸ
<|startofthink|>```JSON\n{\n   "api_name": "style_search_tool",\n    "parameters": {\n      "text": "æˆ‘æƒ³è¦æ¢ä¸ªå¤é£é£æ ¼"\n   }\n}\n```<|endofthink|>

æˆ‘ä¸ºä½ æœç´¢åˆ°çš„é£æ ¼æ˜¯å¤é£é£æ ¼(Old style)ã€‚ä¸‹é¢æ˜¯è¯¥é£æ ¼çš„é¢„è§ˆå›¾ã€‚
æˆ‘ç°åœ¨å°†ç”¨å‰é¢ä½ ä¸Šä¼ çš„ç…§ç‰‡å’Œæ–°é€‰æ‹©çš„é£æ ¼ç”Ÿæˆå†™çœŸç…§ã€‚
ç”Ÿæˆå†™çœŸç…§ä¸­ï¼š
<|startofthink|>```JSON\n{\n   "api_name": "facechain_inference_tool",\n    "parameters": {\n   "matched_style_file_path": "../../styles/leosamsMoonfilm_filmGrain20/Old_style.json"\n  }\n}\n```<|endofthink|>

å¤é£å†™çœŸå·²ç»ç”Ÿæˆå®Œæ¯•ï¼ä½ è¿˜å¯ä»¥ç»§ç»­ç”Ÿæˆè¿™ç±»é£æ ¼çš„ç…§ç‰‡æˆ–è€…æ›´æ¢ä¸€ä¸ªé£æ ¼\n\n</s>

ã€è§’è‰²æ‰®æ¼”è¦æ±‚ã€‘
ä¸Šé¢å¤šè½®è§’è‰²å¯¹è¯æ˜¯æä¾›çš„åˆ›ä½œä¸€ä¸ªå†™çœŸç…§é£æ ¼è¦å’Œç”¨æˆ·æ²Ÿé€šçš„æ ·ä¾‹ï¼Œè¯·æŒ‰ç…§ä¸Šè¿°çš„è¯¢é—®æ­¥éª¤æ¥å¼•å¯¼ç”¨æˆ·å®Œæˆé£æ ¼çš„ç”Ÿæˆï¼Œæ¯æ¬¡åªå›å¤å¯¹åº”çš„å†…å®¹ï¼Œä¸è¦ç”Ÿæˆå¤šè½®å¯¹è¯ã€‚è®°ä½åªå›å¤ç”¨æˆ·å½“å‰çš„æé—®ï¼Œä¸è¦ç”Ÿæˆå¤šè½®å¯¹è¯ï¼Œå›å¤ä¸è¦åŒ…å«<|user|>åé¢çš„å†…å®¹ã€‚

"""


KEY_TEMPLATE = " "
load_dotenv('../config/.env', override=True)
os.environ['TOOL_CONFIG_FILE'] = '../config/cfg_tool_template.json'
os.environ['MODEL_CONFIG_FILE'] = '../config/cfg_model_template.json'
os.environ['OUTPUT_FILE_DIRECTORY'] = './tmp'
dashscope.api_key = os.environ.get('DASHSCOPE_API_KEY')
style_paths = ["../../styles/leosamsMoonfilm_filmGrain20", "../../styles/MajicmixRealistic_v6"]
styles = []
for folder_path in style_paths:
    for filename in os.listdir(folder_path):
        file_path = os.path.join(folder_path, filename)
        with open(file_path, "r") as f:
            data = json.load(f)
            styles.append(data)

with open(
        os.path.join(os.path.dirname(__file__), 'main.css'), "r",
        encoding="utf-8") as f:
    MAIN_CSS_CODE = f.read()

# ----------agent å¯¹è±¡åˆå§‹åŒ–--------------------

tool_cfg_file = os.getenv('TOOL_CONFIG_FILE')
model_cfg_file = os.getenv('MODEL_CONFIG_FILE')

tool_cfg = Config.from_file(tool_cfg_file)
model_cfg = Config.from_file(model_cfg_file)

# model_name = 'modelscope-agent-7b'
# model_name = 'modelscope-agent'
model_name = 'http_llm'
llm = LLMFactory.build_llm(model_name, model_cfg)


def add_file(history, files, uuid_str: str):
    if not uuid_str:
        if os.getenv("MODELSCOPE_ENVIRONMENT") == 'studio':
            raise gr.Error("è¯·ç™»é™†åä½¿ç”¨! (Please login first)")
        else:
            uuid_str = 'facechain_agent'
    history = history + [((file.name,), None) for file in files]
    # task_history = task_history + [((file.name,), None) for file in files]
    file_paths = []
    filtered_list = []
    print(history)
    file_paths = [item[0][0] for item in history if item[0] != None]
    if len(file_paths) == 0:
        raise ValueError("ç…§ç‰‡ä¸Šä¼ å¤±è´¥")
    print("#####", file_paths)
    filtered_list = [item for item in file_paths if '.jpg' in item or '.png' in item]
    print("#####", filtered_list)
    uuid = 'qw'
    # shutil.rmtree(f"./{uuid}", ignore_errors=True)
    base_model_path = 'cv_portrait_model'
    revision = 'v2.0'
    sub_path = "film/film"
    uuid_str=uuid_str[:8]
    print("#####åˆ‡å‰²åuuid_str",uuid_str)
    output_model_name = uuid_str
    output_model_name = slugify.slugify(output_model_name)
    instance_data_dir = os.path.join('./', base_model_path, output_model_name)
    shutil.rmtree(instance_data_dir, ignore_errors=True)
    try:
        prepare_dataset(filtered_list, instance_data_dir)
    except Exception as e:
        raise e("é¢„å¤„ç†å›¾ç‰‡æ•°æ®å‡ºé”™")
    return history


def reset_user_input():
    return gr.update(value="")

#audioè½¬wav
def _preprocess(filename):
    audio_name = 'audio.wav'
    subprocess.call(
        [
            "ffmpeg",
            "-y",
            "-i",
            filename,
            "-acodec",
            "pcm_s16le",
            "-ar",
            "16000",
            "-ac",
            "1",
            "-loglevel",
            "quiet",
            audio_name,
        ]
    )
    return audio_name
#asråˆå§‹åŒ–
recognition = Recognition(model='paraformer-realtime-v1',
                          format='wav',
                          sample_rate=16000,
                          callback=None)

def transcribe(microphone):
    file = microphone
    print(f"\n\nFile is: {file}\n\n")
    print("Starting Preprocessing")
    _preprocess(filename=file)

def process_audio(audio):
    #gradio 3.29æ²¡æœ‰stop_recordingäº‹ä»¶ï¼Œç”¨changeäº‹ä»¶ï¼Œä¼šæœ‰None
    if audio is None:
        return " "
    else:
        transcribe(audio)
        result = recognition.call("audio.wav")
        res = ''
        for sentence in result.get_sentence():
            res +=str(sentence)
        res=eval(res)
        print(res['text'])
        return res['text']

def text_to_speech(text):
    result = SpeechSynthesizer.call(model='sambert-zhichu-v1',
                                    text=text,
                                    sample_rate=48000,
                                    format='wav')
    return result.get_audio_data()
    

    


def init(uuid_str, state):
    uuid_str=uuid_str[:8]
    if not uuid_str:
        if os.getenv("MODELSCOPE_ENVIRONMENT") == 'studio':
            raise gr.Error("è¯·ç™»é™†åä½¿ç”¨! (Please login first)")
        else:
            uuid_str = 'facechain_agent'
    wav_dir = f"./{uuid_str}/wav"
    shutil.rmtree(wav_dir, ignore_errors=True)
    os.makedirs(wav_dir, exist_ok=True)
    style_search_tool = StyleSearchTool(style_paths)
    facechain_finetune_tool = FaceChainFineTuneTool(uuid_str)  # åˆå§‹åŒ–lora_name,åŒºåˆ†ä¸åŒç”¨æˆ·
    facechain_inference_tool = FaceChainInferenceTool(uuid_str)
    additional_tool_list = {
        style_search_tool.name: style_search_tool,
        facechain_finetune_tool.name: facechain_finetune_tool,
        facechain_inference_tool.name: facechain_inference_tool
    }
    prompt_generator = MSPromptGenerator(
        system_template=SYSTEM_PROMPT,
        instruction_template=INSTRUCTION_TEMPLATE)

    prompt_generator_only_gen = MSPromptGenerator(
        system_template=SYSTEM_PROMPT,
        instruction_template=INSTRUCTION_TEMPLATE1)
    agent = AgentExecutor(
        llm,
        tool_cfg,
        prompt_generator=prompt_generator,
        tool_retrieval=False,
        additional_tool_list=additional_tool_list,
        # knowledge_retrieval=knowledge_retrieval
    )
    agent_only_gen = AgentExecutor(
        llm,
        tool_cfg,
        prompt_generator=prompt_generator_only_gen,
        tool_retrieval=False,
        additional_tool_list=additional_tool_list,
        # knowledge_retrieval=knowledge_retrieval
    )
    agent.set_available_tools(additional_tool_list.keys())
    agent_only_gen.set_available_tools(additional_tool_list.keys())
    state['agent'] = agent
    state['agent_only_gen'] = agent_only_gen
    state['wav_dir'] = wav_dir


with gr.Blocks(css=MAIN_CSS_CODE, theme=gr.themes.Soft()) as demo:
    uuid_str = gr.Textbox(label="modelscope_uuid", visible=False)
    state = gr.State({})
    demo.load(init, inputs=[uuid_str, state], outputs=[])
    # ç”Ÿæˆéšæœºçš„ UUIDï¼ˆå’Œuuid=â€˜qw'ä¸ä¸€æ ·ï¼‰

    with gr.Row():
        gr.Markdown(
            "# <center> \N{fire} FaceChain Potrait Generation ([Github star facechain here](https://github.com/modelscope/facechain/tree/main) \N{whale}, [Github star modelscope_agent here](https://github.com/modelscope/modelscope-agent) \N{whale}, [Paper cite facechain here](https://arxiv.org/abs/2308.14256) \N{whale},  [Paper cite modelscope_agent here](https://arxiv.org/abs/2309.00986) \N{whale})</center>")
    with gr.Row():   
        gr.Markdown(
            "##### <center> æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ äº¤æµï¼Œè¯·å‹¿å°†æ¨¡å‹åŠå…¶åˆ¶ä½œå†…å®¹ç”¨äºéæ³•æ´»åŠ¨æˆ–è¿åä»–äººéšç§çš„åœºæ™¯ã€‚(This project is intended solely for the purpose of technological discussion, and should not be used for illegal activities and violating privacy of individuals.)</center>")
    with gr.Row():
        with gr.Column():
            gr.Markdown(""" ğŸŒˆ ğŸŒˆ ğŸŒˆ

                        ## ä½ å¥½ï¼Œæˆ‘æ˜¯FaceChain Agentï¼Œå¯ä»¥å¸®ä½ ç”Ÿæˆå†™çœŸç…§ç‰‡ã€‚ä¸‹é¢æ˜¯å„ç±»é£æ ¼çš„å±•ç¤ºå›¾ï¼Œä½ å¯ä»¥åœ¨è¿™å…ˆæŒ‘é€‰ä½ å–œæ¬¢çš„é£æ ¼ã€‚

                        ## ç„¶ååœ¨ä¸‹æ–¹çš„èŠå¤©æ¡†é‡Œä¸æˆ‘äº¤æµå§ï¼Œä¸€èµ·æ¥ç”Ÿæˆç¾å¦™çš„å†™çœŸç…§ï¼

                        """)
            with gr.Row():
                gallery = gr.Gallery(value=[(os.path.join("../../", item["img"]), item["name"]) for item in styles],
                                     elem_id='gallery', show_label=False
                                     ).style(object_fit='contain', preview=True, columns=6)
    with gr.Row(elem_id="container_row").style(equal_height=True):
        with gr.Column(scale=8, elem_classes=["chatInterface", "chatDialog", "chatContent"]):
            with gr.Row(elem_id='chat-container'):
                chatbot = ChatBot(value=[[None, PROMPT_START]],
                                  elem_id="chatbot",
                                  elem_classes=["markdown-body"],
                                  show_label=False,
                                  )

            with gr.Row(elem_id="chat-bottom-container"):
                with gr.Column(min_width=70, scale=1):
                    clear_session_button = gr.Button(
                        "æ¸…é™¤", elem_id='clear_session_button', default_value=True)
                with gr.Column(scale=12):
                    user_input = gr.Textbox(
                        show_label=False,
                        placeholder="ä¸€èµ·æ¥è‡ªç”±ç”Ÿæˆå†™çœŸç…§å§ï½",
                        elem_id="chat-input").style(container=False)
                with gr.Column(min_width=70, scale=1):
                    submitBtn = gr.Button("å‘é€", variant="primary")
                with gr.Column(min_width=110, scale=1):
                    upload_button = gr.UploadButton("ä¸Šä¼ ç…§ç‰‡", file_types=["image"], file_count="multiple")
                with gr.Column(min_width=110, scale=1):
                    regenerate_button = gr.Button(
                        "é‡æ–°ç”Ÿæˆ", elem_id='regenerate_button')
            with gr.Row(elem_id="chat-bottom-container"):
                with gr.Column(scale=12):
                    audio = gr.Audio(source='microphone',type="filepath",label='è¯­éŸ³è¾“å…¥')
            gr.Examples(
                examples=['æˆ‘æƒ³è¦ç‰›ä»”é£', 'æˆ‘æƒ³è¦å‡¤å† éœå¸”é£', 'æˆ‘çš„ç…§ç‰‡ä¸Šä¼ å¥½äº†', 'æˆ‘æƒ³æ¢æˆå·¥ä½œé£'],
                inputs=[user_input],
                label="ç¤ºä¾‹",
                elem_id="chat-examples")

    
    def facechain_agent(*inputs):

        user_input = inputs[0]
        chatbot = inputs[1]
        state = inputs[2]
        agent = state['agent']

        chatbot.append((user_input, None))
        yield chatbot
        history_image = []

        def update_component(exec_result, history):
            exec_result = exec_result['result']
            name = exec_result.get('name')
            if name == 'facechain_inference_tool':
                single_path = exec_result['single_path']
                image_files = glob.glob(os.path.join(single_path, '*.jpg'))
                image_files += glob.glob(os.path.join(single_path, '*.png'))
                history = [(None, (file,)) for file in image_files]
            else:
                history = []
            return history

        def preview_image(exec_result, history):
            print("####################### exec_result", exec_result)
            exec_result = exec_result['result']
            name = exec_result['name']
            if name == 'style_search_tool':
                preview_image_path = exec_result['file_path']
                with open(preview_image_path, "r") as f:
                    data = json.load(f)
                preview_image = os.path.join("../../",data["img"])
                history = [(None,(preview_image,))]
                
            else:
                history = [] 
            return history

        response = ''
        i = 0 #i,j,mæ§åˆ¶è¯­éŸ³è¾“å‡ºé€»è¾‘
        j = 0
        k = -1 #kæ§åˆ¶æ–‡æœ¬è¾“å‡ºä½ç½®
        m = 0
        for frame in agent.stream_run(user_input + KEY_TEMPLATE, remote=True):
            global l
            is_final = frame.get("frame_is_final")
            llm_result = frame.get("llm_text", "")
            exec_result = frame.get('exec_result', '')
            history = []
            
            llm_result = llm_result.split("<|user|>")[0].strip()
            if len(exec_result) != 0:
                preview_image_history = preview_image(exec_result, chatbot)
                history = update_component(exec_result, chatbot)
                frame_text = " "
            else:
                # action_exec_result
                frame_text = llm_result
                response = f'{response}\n{frame_text}'
                
                chatbot[k] = (user_input, response)
            if history != []:
                history_image = history
            # if preview_image_history != []:
            #     pre_image = preview_image_history
            yield chatbot
            print(response)
            wav_dir = state['wav_dir']
            if i == 0:
                index1 = response.find("<|startofthink|>")
                text1 = response[:index1]
                data = text_to_speech(text1)
                with open(f'{wav_dir}/text1.wav', 'wb') as f:
                    f.write(data)
                chatbot.append((None,(f'{wav_dir}/text1.wav',)))
                i = 1
                k -= 1
                yield chatbot
            
            if j == 0: 
                index2 = response.find("<|endofthink|>")
                text2 = response[index2:].replace("<|endofthink|>"," ",1)
                if text2 != " ":
                    index3 = text2.find("<|startofthink|>")
                    index4 = text2.find("<|endofthink|>")
                    text4 = text2[index4:].replace("<|endofthink|>"," ")
                    if index3 != -1:
                        if text4 == " " and m == 0:
                            text3 = text2[:index3]
                            data = text_to_speech(text3)
                            with open(f'{wav_dir}/text3.wav', 'wb') as f:
                                f.write(data)
                            chatbot.append((None,(f'{wav_dir}/text3.wav',)))
                            k -= 1
                            yield chatbot
                            m = 1
                            try:
                                if preview_image_history !=[]:
                                    for item in preview_image_history:
                                        chatbot.append(item)
                                        yield chatbot
                                        k -= 1
                                    preview_image_history = []
                            except:
                                pass 
                            
                        if text4 != " ":
                            data = text_to_speech(text4)
                            with open(f'{wav_dir}/text4.wav', 'wb') as f:
                                f.write(data)
                            chatbot.append((None,(f'{wav_dir}/text4.wav',)))
                            k -= 1
                            yield chatbot
                            j =1  
                    else:
                        data = text_to_speech(text2)
                        with open(f'{wav_dir}/text2.wav', 'wb') as f:
                                f.write(data)
                        chatbot.append((None,(f'{wav_dir}/text2.wav',)))
                        k -= 1
                        j =1
                        yield chatbot 
                        try:
                            if preview_image_history !=[]:
                                for item in preview_image_history:
                                    chatbot.append(item)
                                    yield chatbot
                                    k -= 1
                                preview_image_history = []
                        except:
                            pass  
                else:
                    pass 
                      
        
        try:
            if history_image != []:
                # make sure gen only for agent

                try:
                    inputs[2]['agent'] = state['agent_only_gen']
                except Exception as e:
                    import traceback
                    print(f'error {e} with detail {traceback.format_exc()}')
                    
                for item in history_image:
                    chatbot.append(item)
                    yield chatbot

        except:
            pass
           


    # ---------- äº‹ä»¶ ---------------------

    stream_predict_input = [user_input, chatbot, state]
    stream_predict_output = [chatbot]

    clean_outputs_start = ['', gr.update(value=[(None, PROMPT_START)])]
    clean_outputs = ['', gr.update(value=[])]
    clean_outputs_target = [user_input, chatbot]
    user_input.submit(
        facechain_agent,
        inputs=stream_predict_input,
        outputs=stream_predict_output,
        show_progress=True)
    user_input.submit(
        fn=lambda: clean_outputs, inputs=[], outputs=clean_outputs_target)
    submitBtn.click(
        facechain_agent,
        stream_predict_input,
        stream_predict_output,
        show_progress=True
    )
    submitBtn.click(reset_user_input, [], [user_input])
    regenerate_button.click(
        fn=lambda: clean_outputs, inputs=[], outputs=clean_outputs_target)
    regenerate_button.click(
        facechain_agent,
        stream_predict_input,
        stream_predict_output,
        show_progress=True)


    def clear_session(state):
        agent = state['agent']
        agent.reset()


    clear_session_button.click(fn=clear_session, inputs=[state], outputs=[])
    clear_session_button.click(
        fn=lambda: clean_outputs_start, inputs=[], outputs=clean_outputs_target)
    upload_button.upload(add_file, inputs=[chatbot, upload_button, uuid_str], outputs=[chatbot], show_progress=True)
    audio.change(process_audio,inputs=[audio],outputs=[user_input])
demo.title = "Facechian Agent ğŸ"
if __name__ == "__main__":
    # print(multiprocessing.get_start_method())
    multiprocessing.set_start_method('spawn')
    demo.queue(status_update_rate=1).launch(share=True)
